<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI on Hardware</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
        h1, h2, h3 { color: #333; }
        ul { list-style-type: square; }
        table { width: 100%; border-collapse: collapse; margin-top: 10px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f4f4f4; }
    </style>
</head>
<body>
    <h1>AI on Hardware</h1>
    <h2>Ramin Anushiravani + LLM</h2>
    
    <h2>Calculating Power Consumption</h2>
    <h3>1. Analytical Estimation:</h3>
    <ul>
        <li><strong>Operation Counting:</strong> Count the number of key operations (e.g., multiply–accumulate operations or MACs) performed during inference.</li>
        <li><strong>Memory Access Costs:</strong> Evaluate energy costs from loading weights and activations from memory.</li>
        <li><strong>Combined Models:</strong> Sum both computation and memory energy estimates to get the overall power consumption per inference.</li>
    </ul>
    
    <h3>2. Empirical Measurement:</h3>
    <ul>
        <li><strong>Hardware Counters and Tools:</strong> Use performance monitoring tools (e.g., RAPL on CPUs, vendor-specific counters on NPUs).</li>
        <li><strong>Profiling Frameworks:</strong> Tools like NeuralPower or frameworks within TensorFlow can help profile the energy footprint of ML models.</li>
    </ul>
    
    <h2>Typical Power Consumption in Wearable Devices</h2>
    <ul>
        <li><strong>Wearable System Power Budget:</strong> Many wearable devices operate on batteries with capacities around 200–500 mAh at ~3.7 V.</li>
        <li><strong>Component-Level Consumption:</strong>
            <ul>
                <li><strong>Sensors:</strong> Often in the microwatt (µW) to low milliwatt (mW) range.</li>
                <li><strong>Wireless Communication:</strong> Maintaining a Bluetooth connection might add tens to a few hundred mW.</li>
                <li><strong>Processing Units:</strong> General-purpose processors can be power-hungry, while dedicated NPUs or accelerators are more energy-efficient.</li>
            </ul>
        </li>
    </ul>
    
    <h2>Optimization Strategies Based on Power Constraints</h2>
    <table>
        <tr>
            <th>Aspect</th>
            <th>Challenge</th>
            <th>Optimization Strategy</th>
        </tr>
        <tr>
            <td>Computation</td>
            <td>High number of MACs increases dynamic power consumption</td>
            <td>Reduce model complexity (e.g., use lightweight architectures like MobileNet), quantize model parameters</td>
        </tr>
        <tr>
            <td>Memory Access</td>
            <td>Frequent loading of weights/activations from off-chip memory is costly</td>
            <td>Employ operator fusion, prune redundant connections</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Wireless data transmission (e.g., Bluetooth) consumes significant power</td>
            <td>Minimize data transmission, use efficient data compression</td>
        </tr>
        <tr>
            <td>Hardware Utilization</td>
            <td>General-purpose processors are less energy-efficient</td>
            <td>Offload to NPUs, use dynamic voltage and frequency scaling</td>
        </tr>
    </table>
    
    <h2>Measuring Throughput and Latency</h2>
    <ul>
        <li><strong>Throughput Measurement:</strong> Run the model repeatedly and count inferences per second.</li>
        <li><strong>Latency Measurement:</strong> Time a single inference using profiling tools or built-in timers.</li>
    </ul>
    
    <h2>Memory Bandwidth</h2>
    <p>Memory Bandwidth is the maximum rate at which a device can read from or write to its memory.</p>
    <h3>Typical Values:</h3>
    <ul>
        <li><strong>Mobile/Embedded Devices:</strong> 5–30 GB/s</li>
        <li><strong>Desktop CPUs/GPUs:</strong> 20–100 GB/s (CPUs), 200–300 GB/s (high-end GPUs)</li>
    </ul>
    
    <h3>Optimization Strategies:</h3>
    <ul>
        <li><strong>Operator Fusion:</strong> Combine sequential operations into a single kernel.</li>
        <li><strong>Quantization & Pruning:</strong> Reduce model weight size to lessen memory demands.</li>
        <li><strong>Efficient Memory Access Patterns:</strong> Use tiling and optimized data layout to improve cache usage.</li>
    </ul>

    <h1>Knowledge Distillation</h1>
    <h2>Ramin Anushiravani + LLMs</h2>
    
    <h3>1. Introduction to Knowledge Distillation</h3>
    <p>Knowledge Distillation (KD) is a model compression technique where a smaller "student" model is trained to mimic the behavior of a larger "teacher" model.</p>
    <h4>Key Concepts:</h4>
    <ul>
        <li><strong>Teacher Model:</strong> A pre-trained, high-accuracy model.</li>
        <li><strong>Student Model:</strong> A smaller model that learns from the teacher.</li>
        <li><strong>Soft Targets:</strong> Probabilities or logits from the teacher model.</li>
        <li><strong>Distillation Loss:</strong> A combination of standard loss and KL divergence.</li>
    </ul>
    
    <h3>2. Motivation and Benefits</h3>
    <ul>
        <li>Model Compression</li>
        <li>Inference Efficiency</li>
        <li>Improved Generalization</li>
        <li>Flexibility</li>
    </ul>
    
    <h3>3. Core Techniques in Knowledge Distillation</h3>
    <h4>3.1 Logit Matching (Soft Targets)</h4>
    <p>Uses temperature scaling to soften output distribution.</p>
    <h4>3.2 Feature-Based Distillation</h4>
    <p>Aligns intermediate representations between teacher and student.</p>
    <h4>3.3 Relation-Based Distillation</h4>
    <p>Transfers relational knowledge like pairwise distances.</p>
    
    <h3>4. Distillation Loss Formulation</h3>
    <p>Uses a combination of standard task loss (e.g., cross-entropy) and distillation loss (e.g., KL divergence).</p>
    
    <h3>5. Practical Steps to Implement Knowledge Distillation</h3>
    <ul>
        <li>Train or select a teacher model.</li>
        <li>Design a smaller, efficient student model.</li>
        <li>Define the distillation loss.</li>
        <li>Combine with standard classification loss.</li>
        <li>Train the student model using both hard labels and soft targets.</li>
        <li>Evaluate and fine-tune the student model.</li>
    </ul>
    
    <h3>6. Impact on Hardware and Performance</h3>
    <ul>
        <li>Reduced Model Size</li>
        <li>Faster Inference</li>
        <li>Lower Energy Consumption</li>
    </ul>
    
    <h3>7. Advanced Techniques and Variations</h3>
    <h4>Ensemble Distillation</h4>
    <p>Uses multiple teacher models.</p>
    <h4>Self-Distillation</h4>
    <p>A model is its own teacher in iterative training.</p>
    <h4>Multi-Task and Cross-Modal Distillation</h4>
    <p>Transfers knowledge across tasks or modalities.</p>
    
    <h3>8. Real-World Applications</h3>
    <ul>
        <li>Deploying on Edge Devices</li>
        <li>Improving Generalization</li>
        <li>Accelerating Inference in Data Centers</li>
    </ul>
    
    <h3>9. Final Takeaways</h3>
    <p>Knowledge Distillation is a powerful method for model compression and efficiency improvement, making AI models more practical for real-world applications.</p>


  
</body>
</html>

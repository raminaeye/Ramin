<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI on Hardware Tutorials</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #333; }
    ul { list-style-type: square; }
    table { width: 100%; border-collapse: collapse; margin-top: 10px; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    th { background-color: #f4f4f4; }
    /* Navigation styling */
    nav { margin-bottom: 30px; padding: 10px; background: #f9f9f9; border: 1px solid #ddd; }
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #007BFF; }
    nav a:hover { text-decoration: underline; }
    /* Section spacing */
    section { margin-bottom: 50px; }
  </style>
</head>
<body>
  <h1>AI on Hardware Tutorials</h1>
  
  <nav>
    <ul>
      <li><a href="#powerConsumption">Power Consumption</a></li>
      <li><a href="#knowledgeDistillation">Knowledge Distillation</a></li>
      <li><a href="#pruning">Pruning</a></li>
      <li><a href="#quantization">Quantization</a></li>
    </ul>
  </nav>

  <!-- Calculating Power Consumption Section -->
  <section id="powerConsumption">
    <h2>Calculating Power Consumption</h2>
    <h3>1. Analytical Estimation:</h3>
    <ul>
      <li><strong>Operation Counting:</strong> Count the number of key operations (e.g., multiply–accumulate operations or MACs) performed during inference.</li>
      <li><strong>Memory Access Costs:</strong> Evaluate energy costs from loading weights and activations from memory.</li>
      <li><strong>Combined Models:</strong> Sum both computation and memory energy estimates to get the overall power consumption per inference.</li>
    </ul>
    <h3>2. Empirical Measurement:</h3>
    <ul>
      <li><strong>Hardware Counters and Tools:</strong> Use performance monitoring tools (e.g., RAPL on CPUs, vendor-specific counters on NPUs).</li>
      <li><strong>Profiling Frameworks:</strong> Tools like NeuralPower or frameworks within TensorFlow can help profile the energy footprint of ML models.</li>
    </ul>
    <h2>Typical Power Consumption in Wearable Devices</h2>
    <ul>
      <li><strong>Wearable System Power Budget:</strong> Many wearable devices operate on batteries with capacities around 200–500 mAh at ~3.7 V.</li>
      <li><strong>Component-Level Consumption:</strong>
        <ul>
          <li><strong>Sensors:</strong> Often in the microwatt (µW) to low milliwatt (mW) range.</li>
          <li><strong>Wireless Communication:</strong> Maintaining a Bluetooth connection might add tens to a few hundred mW.</li>
          <li><strong>Processing Units:</strong> General-purpose processors can be power-hungry, while dedicated NPUs or accelerators are more energy-efficient.</li>
        </ul>
      </li>
    </ul>
    <h2>Optimization Strategies Based on Power Constraints</h2>
    <table>
      <tr>
        <th>Aspect</th>
        <th>Challenge</th>
        <th>Optimization Strategy</th>
      </tr>
      <tr>
        <td>Computation</td>
        <td>High number of MACs increases dynamic power consumption</td>
        <td>Reduce model complexity (e.g., use lightweight architectures like MobileNet), quantize model parameters</td>
      </tr>
      <tr>
        <td>Memory Access</td>
        <td>Frequent loading of weights/activations from off-chip memory is costly</td>
        <td>Employ operator fusion, prune redundant connections</td>
      </tr>
      <tr>
        <td>Communication</td>
        <td>Wireless data transmission (e.g., Bluetooth) consumes significant power</td>
        <td>Minimize data transmission, use efficient data compression</td>
      </tr>
      <tr>
        <td>Hardware Utilization</td>
        <td>General-purpose processors are less energy-efficient</td>
        <td>Offload to NPUs, use dynamic voltage and frequency scaling</td>
      </tr>
    </table>
    <h2>Measuring Throughput and Latency</h2>
    <ul>
      <li><strong>Throughput Measurement:</strong> Run the model repeatedly and count inferences per second.</li>
      <li><strong>Latency Measurement:</strong> Time a single inference using profiling tools or built-in timers.</li>
    </ul>
    <h2>Memory Bandwidth</h2>
    <p>Memory Bandwidth is the maximum rate at which a device can read from or write to its memory.</p>
    <h3>Typical Values:</h3>
    <ul>
      <li><strong>Mobile/Embedded Devices:</strong> 5–30 GB/s</li>
      <li><strong>Desktop CPUs/GPUs:</strong> 20–100 GB/s (CPUs), 200–300 GB/s (high-end GPUs)</li>
    </ul>
    <h3>Optimization Strategies:</h3>
    <ul>
      <li><strong>Operator Fusion:</strong> Combine sequential operations into a single kernel.</li>
      <li><strong>Quantization & Pruning:</strong> Reduce model weight size to lessen memory demands.</li>
      <li><strong>Efficient Memory Access Patterns:</strong> Use tiling and optimized data layout to improve cache usage.</li>
    </ul>
  </section>

  <!-- Knowledge Distillation Section -->
  <section id="knowledgeDistillation">
    <h2>Knowledge Distillation</h2>
    <h3>1. Introduction to Knowledge Distillation</h3>
    <p>Knowledge Distillation (KD) is a model compression technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model.</p>
    <h4>Key Concepts:</h4>
    <ul>
      <li><strong>Teacher Model:</strong> A pre-trained, high-accuracy model.</li>
      <li><strong>Student Model:</strong> A smaller model that learns from the teacher.</li>
      <li><strong>Soft Targets:</strong> Probabilities or logits from the teacher model.</li>
      <li><strong>Distillation Loss:</strong> A combination of standard loss and KL divergence.</li>
    </ul>
    <h3>2. Motivation and Benefits</h3>
    <ul>
      <li>Model Compression</li>
      <li>Inference Efficiency</li>
      <li>Improved Generalization</li>
      <li>Flexibility</li>
    </ul>
    <h3>3. Core Techniques in Knowledge Distillation</h3>
    <h4>3.1 Logit Matching (Soft Targets)</h4>
    <p>Uses temperature scaling to soften output distribution.</p>
    <h4>3.2 Feature-Based Distillation</h4>
    <p>Aligns intermediate representations between teacher and student.</p>
    <h4>3.3 Relation-Based Distillation</h4>
    <p>Transfers relational knowledge like pairwise distances.</p>
    <h3>4. Distillation Loss Formulation</h3>
    <p>Uses a combination of standard task loss (e.g., cross-entropy) and distillation loss (e.g., KL divergence).</p>
    <h3>5. Practical Steps to Implement Knowledge Distillation</h3>
    <ul>
      <li>Train or select a teacher model.</li>
      <li>Design a smaller, efficient student model.</li>
      <li>Define the distillation loss.</li>
      <li>Combine with standard classification loss.</li>
      <li>Train the student model using both hard labels and soft targets.</li>
      <li>Evaluate and fine-tune the student model.</li>
    </ul>
    <h3>6. Impact on Hardware and Performance</h3>
    <ul>
      <li>Reduced Model Size</li>
      <li>Faster Inference</li>
      <li>Lower Energy Consumption</li>
    </ul>
    <h3>7. Advanced Techniques and Variations</h3>
    <h4>Ensemble Distillation</h4>
    <p>Uses multiple teacher models.</p>
    <h4>Self-Distillation</h4>
    <p>A model is its own teacher in iterative training.</p>
    <h4>Multi-Task and Cross-Modal Distillation</h4>
    <p>Transfers knowledge across tasks or modalities.</p>
    <h3>8. Real-World Applications</h3>
    <ul>
      <li>Deploying on Edge Devices</li>
      <li>Improving Generalization</li>
      <li>Accelerating Inference in Data Centers</li>
    </ul>
    <h3>9. Final Takeaways</h3>
    <p>Knowledge Distillation is a powerful method for model compression and efficiency improvement, making AI models more practical for real-world applications.</p>
  </section>

  <!-- Pruning Section -->
  <section id="pruning">
    <h2>Pruning</h2>
    <h3>Ramin Anushiravani + LLMs</h3>
    <h3>1. Introduction</h3>
    <p>Modern deep neural networks are typically over-parameterized, leading to unnecessary computational and memory overhead. Pruning is a model compression technique aimed at eliminating redundant weights and neurons, yielding a sparse model that is lighter and more efficient.</p>
    <h3>2. Understanding Hardware Constraints</h3>
    <h4>Hardware Characteristics</h4>
    <ul>
      <li><strong>General-Purpose GPUs and CPUs:</strong>
        <ul>
          <li>Traditionally optimized for dense linear algebra.</li>
          <li>Unstructured sparsity may reduce model size but may not always yield runtime gains.</li>
          <li>Structured pruning removes entire neurons, channels, or filters for better efficiency.</li>
        </ul>
      </li>
      <li><strong>Specialized Accelerators & Edge Devices:</strong>
        <ul>
          <li>Some hardware supports unstructured sparsity for energy efficiency.</li>
          <li>For mobile or embedded devices, structured pruning is often preferred.</li>
        </ul>
      </li>
    </ul>
    <h4>Matching Pruning Strategy to Hardware</h4>
    <ul>
      <li>Determine if hardware supports sparse operations efficiently.</li>
      <li>Consider inference latency, energy consumption, and computational savings.</li>
    </ul>
    <h3>3. Overview of Pruning Techniques</h3>
    <h4>Basic Techniques</h4>
    <ul>
      <li><strong>Magnitude-Based Pruning:</strong> Removes weights below a certain threshold.</li>
      <li><strong>One-Shot vs. Iterative Pruning:</strong>
        <ul>
          <li>One-Shot: Removes a fixed percentage of weights in a single step.</li>
          <li>Iterative: Prunes gradually with retraining in between.</li>
        </ul>
      </li>
    </ul>
    <h4>Advanced Techniques</h4>
    <ul>
      <li>Adaptive thresholding for layer-specific pruning.</li>
      <li>Dynamic pruning schedules during training.</li>
      <li>Regularization-based pruning using L1 regularization.</li>
    </ul>
    <h3>4. Step-by-Step Pruning Process</h3>
    <h4>Step 1: Establish a Baseline</h4>
    <ul>
      <li>Evaluate model accuracy, latency, and resource utilization.</li>
      <li>Set goals such as reducing model size or improving efficiency.</li>
    </ul>
    <h4>Step 2: Select an Appropriate Pruning Strategy</h4>
    <ul>
      <li>Choose structured or unstructured pruning based on hardware constraints.</li>
      <li>Opt for iterative pruning with dynamic schedules.</li>
    </ul>
    <h4>Step 3: Implement Pruning</h4>
    <ul>
      <li>Use libraries like TensorFlow Model Optimization Toolkit or PyTorch.</li>
      <li>Apply pruning through weight masking and gradual sparsity increments.</li>
    </ul>
    <h4>Step 4: Fine-Tuning and Retraining</h4>
    <ul>
      <li>Retrain after each pruning round to recover accuracy.</li>
      <li>Monitor accuracy, sparsity, and convergence.</li>
    </ul>
    <h3>5. Evaluating the Impact</h3>
    <ul>
      <li>Analyze accuracy vs. sparsity trade-offs.</li>
      <li>Measure inference latency, memory usage, and energy consumption.</li>
    </ul>
    <h3>6. Integrating Pruning with Other Compression Techniques</h3>
    <ul>
      <li>Combine pruning with quantization for better efficiency.</li>
      <li>Use Neural Architecture Search (NAS) for optimized architectures.</li>
    </ul>
    <h3>7. Final Recommendations and Best Practices</h3>
    <ul>
      <li>Start with modest pruning ratios and gradually increase them.</li>
      <li>Use iterative pruning with fine-tuning to maintain accuracy.</li>
      <li>Evaluate pruned models on target hardware.</li>
    </ul>
    <h3>Conclusion</h3>
    <p>Pruning is a powerful technique for model optimization, balancing sparsity and performance. Understanding both model and hardware constraints is crucial to achieving meaningful efficiency gains.</p>
    <h3>Reference</h3>
    <ul>
      <li><a href="https://www.dropbox.com/scl/fi/6qspcmk8qayy7mft737gh/Lec03-Pruning-I.pdf">Lec03: Pruning I</a></li>
      <li><a href="https://www.dropbox.com/scl/fi/w5baiyci5cxl1ozpy6lsr/Lec04-Pruning-II.pdf">Lec04: Pruning II</a></li>
    </ul>
  </section>

  <!-- Quantization Section -->
  <section id="quantization">
    <h2>Quantization</h2>
    <h3>Ramin Anushiravani + LLMs</h3>
    <p>Imagine you have a powerful model with millions of parameters, and now it’s time to deploy it. How can you compress it without sacrificing accuracy? One essential technique to consider is quantization, a key step in optimizing models for deployment.</p>
    <h3>1. Introduction</h3>
    <p>Deep neural networks are notorious for their large model sizes and high computational requirements. Quantization offers a powerful solution by reducing the numerical precision of model parameters (weights) and activations—from high-precision (e.g., 32-bit floating point) to lower-precision formats (e.g., 8-bit integers). This process not only shrinks the model size but also speeds up inference and lowers energy consumption, making it particularly valuable for deployment on resource-constrained hardware.</p>
    <h3>2. Motivation and Impact</h3>
    <h4>2.1. Why Quantize?</h4>
    <ul>
      <li><strong>Model Compression:</strong> Reduces storage requirements, crucial for mobile devices, embedded systems, and edge computing.</li>
      <li><strong>Inference Acceleration:</strong> Lower precision arithmetic (e.g., 8-bit integer operations) typically executes faster than 32-bit floating-point operations.</li>
      <li><strong>Energy Efficiency:</strong> Reducing bit-width lowers energy per operation, benefiting battery-powered devices and large-scale data centers.</li>
      <li><strong>Complementary to Pruning:</strong> When combined with pruning, quantization further compresses the model, improving computational efficiency.</li>
    </ul>
    <h4>2.2. Hardware Considerations</h4>
    <ul>
      <li><strong>General-Purpose CPUs/GPUs:</strong> Optimized for dense matrix operations but require software tuning to exploit lower precision arithmetic.</li>
      <li><strong>Edge Devices & Mobile Hardware:</strong> Often include dedicated low-precision processing units (e.g., ARM’s NEON, DSPs) for efficient execution.</li>
      <li><strong>Specialized Accelerators:</strong> Custom AI accelerators and FPGAs support mixed or ultra-low precision, making them ideal for highly quantized models.</li>
    </ul>
    <h3>3. Core Concepts in Quantization</h3>
    <h4>3.1. Precision Reduction</h4>
    <ul>
      <li><strong>Fixed-Point Representation:</strong> Converts 32-bit floating point values to lower-bit fixed-point numbers.</li>
      <li><strong>Quantization Error:</strong> Lowering precision introduces noise, which can degrade accuracy if not managed properly.</li>
    </ul>
    <h4>3.2. Types of Quantization</h4>
    <ul>
      <li><strong>Post-Training Quantization (PTQ):</strong> Converts a pre-trained model to a quantized version without retraining.</li>
      <li><strong>Quantization-Aware Training (QAT):</strong> Simulates quantization during training so the model adapts to lower precision.</li>
    </ul>
    <h3>4. Advanced Quantization Techniques</h3>
    <ul>
      <li><strong>Mixed-Precision Quantization:</strong> Assigns different bit-widths to different layers based on sensitivity.</li>
      <li><strong>Non-Uniform Quantization:</strong> Allocates bins to better fit the distribution of weights and activations.</li>
      <li><strong>Per-Channel vs. Per-Tensor Quantization:</strong> Uses different scaling factors per channel for improved accuracy.</li>
    </ul>
    <h3>5. Implementation Steps</h3>
    <ol>
      <li><strong>Baseline Evaluation:</strong> Benchmark the original model’s accuracy, size, and latency.</li>
      <li><strong>Strategy Selection:</strong> Choose between PTQ and QAT.</li>
      <li><strong>Calibration and Parameter Estimation:</strong> Collect data to determine dynamic ranges of activations and weights.</li>
      <li><strong>Applying Quantization:</strong> Use TensorFlow Model Optimization Toolkit or PyTorch’s quantization modules.</li>
      <li><strong>Fine-Tuning and Iteration:</strong> Adjust learning rates and calibration parameters.</li>
      <li><strong>Hardware Deployment and Testing:</strong> Validate and optimize on the target hardware.</li>
    </ol>
    <h3>6. Performance and Hardware Impact</h3>
    <ul>
      <li><strong>Inference Speed and Latency:</strong> Quantized models execute faster, improving real-time performance.</li>
      <li><strong>Memory and Energy Efficiency:</strong> Reduces memory footprint and extends battery life.</li>
      <li><strong>Accuracy Considerations:</strong> Techniques like mixed-precision help maintain model accuracy.</li>
    </ul>
    <h3>7. Integration with Other Compression Techniques</h3>
    <ul>
      <li><strong>Combining with Pruning:</strong> Reduces both the number of parameters and computational complexity.</li>
      <li><strong>End-to-End Compression Pipelines:</strong> Includes distillation, low-rank approximations, and neural architecture search (NAS).</li>
    </ul>
    <h3>8. Best Practices and Recommendations</h3>
    <ul>
      <li>Start with a baseline and define clear objectives.</li>
      <li>Use iterative approaches with gradual precision reduction.</li>
      <li>Leverage hardware capabilities for optimized quantization strategies.</li>
      <li>Monitor and validate performance metrics continuously.</li>
    </ul>
    <h3>9. Conclusion</h3>
    <p>Quantization is a critical tool in model compression, reducing model size, improving inference speed, and lowering energy usage. Whether using post-training quantization or quantization-aware training, careful calibration, fine-tuning, and hardware validation ensure optimal results. Combining quantization with techniques like pruning further enhances model efficiency for real-world deployment.</p>
  </section>
  
</body>
</html>

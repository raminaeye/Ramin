<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI on Hardware</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
        h1, h2, h3 { color: #333; }
        ul { list-style-type: square; }
        table { width: 100%; border-collapse: collapse; margin-top: 10px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f4f4f4; }
    </style>
</head>
<body>
    <h1>AI on Hardware</h1>
    <h2>Ramin Anushiravani + LLM</h2>
    
    <h2>Calculating Power Consumption</h2>
    <h3>1. Analytical Estimation:</h3>
    <ul>
        <li><strong>Operation Counting:</strong> Count the number of key operations (e.g., multiply–accumulate operations or MACs) performed during inference.</li>
        <li><strong>Memory Access Costs:</strong> Evaluate energy costs from loading weights and activations from memory.</li>
        <li><strong>Combined Models:</strong> Sum both computation and memory energy estimates to get the overall power consumption per inference.</li>
    </ul>
    
    <h3>2. Empirical Measurement:</h3>
    <ul>
        <li><strong>Hardware Counters and Tools:</strong> Use performance monitoring tools (e.g., RAPL on CPUs, vendor-specific counters on NPUs).</li>
        <li><strong>Profiling Frameworks:</strong> Tools like NeuralPower or frameworks within TensorFlow can help profile the energy footprint of ML models.</li>
    </ul>
    
    <h2>Typical Power Consumption in Wearable Devices</h2>
    <ul>
        <li><strong>Wearable System Power Budget:</strong> Many wearable devices operate on batteries with capacities around 200–500 mAh at ~3.7 V.</li>
        <li><strong>Component-Level Consumption:</strong>
            <ul>
                <li><strong>Sensors:</strong> Often in the microwatt (µW) to low milliwatt (mW) range.</li>
                <li><strong>Wireless Communication:</strong> Maintaining a Bluetooth connection might add tens to a few hundred mW.</li>
                <li><strong>Processing Units:</strong> General-purpose processors can be power-hungry, while dedicated NPUs or accelerators are more energy-efficient.</li>
            </ul>
        </li>
    </ul>
    
    <h2>Optimization Strategies Based on Power Constraints</h2>
    <table>
        <tr>
            <th>Aspect</th>
            <th>Challenge</th>
            <th>Optimization Strategy</th>
        </tr>
        <tr>
            <td>Computation</td>
            <td>High number of MACs increases dynamic power consumption</td>
            <td>Reduce model complexity (e.g., use lightweight architectures like MobileNet), quantize model parameters</td>
        </tr>
        <tr>
            <td>Memory Access</td>
            <td>Frequent loading of weights/activations from off-chip memory is costly</td>
            <td>Employ operator fusion, prune redundant connections</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Wireless data transmission (e.g., Bluetooth) consumes significant power</td>
            <td>Minimize data transmission, use efficient data compression</td>
        </tr>
        <tr>
            <td>Hardware Utilization</td>
            <td>General-purpose processors are less energy-efficient</td>
            <td>Offload to NPUs, use dynamic voltage and frequency scaling</td>
        </tr>
    </table>
    
    <h2>Measuring Throughput and Latency</h2>
    <ul>
        <li><strong>Throughput Measurement:</strong> Run the model repeatedly and count inferences per second.</li>
        <li><strong>Latency Measurement:</strong> Time a single inference using profiling tools or built-in timers.</li>
    </ul>
    
    <h2>Memory Bandwidth</h2>
    <p>Memory Bandwidth is the maximum rate at which a device can read from or write to its memory.</p>
    <h3>Typical Values:</h3>
    <ul>
        <li><strong>Mobile/Embedded Devices:</strong> 5–30 GB/s</li>
        <li><strong>Desktop CPUs/GPUs:</strong> 20–100 GB/s (CPUs), 200–300 GB/s (high-end GPUs)</li>
    </ul>
    
    <h3>Optimization Strategies:</h3>
    <ul>
        <li><strong>Operator Fusion:</strong> Combine sequential operations into a single kernel.</li>
        <li><strong>Quantization & Pruning:</strong> Reduce model weight size to lessen memory demands.</li>
        <li><strong>Efficient Memory Access Patterns:</strong> Use tiling and optimized data layout to improve cache usage.</li>
    </ul>


  
</body>
</html>

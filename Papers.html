<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What I'm Reading | Ramin Anushiravani</title>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 0; 
      background-color: #f4f4f4; 
      line-height: 1.6;
      color: #333;
    }
    .container { 
      max-width: 900px; 
      margin: 20px auto; 
      background: #fff; 
      padding: 20px; 
      border-radius: 8px; 
      box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
    }
    header { text-align: center; padding-bottom: 20px; border-bottom: 1px solid #ddd; }
    nav { text-align: center; margin-top: 10px; }
    nav a { margin: 0 10px; text-decoration: none; color: #0073e6; font-weight: bold; }
    h2 { border-bottom: 2px solid #0073e6; padding-bottom: 5px; }
    ul { list-style-type: none; padding: 0; }
    ul li { margin: 10px 0; }
    .paper { margin-bottom: 20px; }
    footer { text-align: center; margin-top: 40px; font-size: 0.9em; color: #777; }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>What I'm Reading</h1>
      <nav>
        <a href="index.html">Home</a>
        <a href="projects.html">What I'm Reading</a>
      </nav>
    </header>

    <section>
      <h2>Recent Papers & Summaries</h2>
      <div class="paper">
        <h3>Open-Set Sound Event Classification using Self-Supervised Learning</h3>
        <p><strong>Analogy:</strong> A security system that recognizes known sounds but flags unknown ones.</p>
        <p><strong>Novelty & Interest:</strong> Uses deep learning to form a compact “map” of known sounds while detecting new ones.</p>
        <p><strong>Conclusion:</strong> Successfully enhances recognition accuracy for both familiar and novel sound events.</p>
      </div>
      <div class="paper">
        <h3>Show and Tell: A Neural Image Caption Generator</h3>
        <p><strong>Analogy:</strong> A bilingual tour guide describing landmarks in vivid detail.</p>
        <p><strong>Novelty & Interest:</strong> Combines CNN and RNN to bridge vision and language.</p>
        <p><strong>Conclusion:</strong> Generates accurate, fluent captions, advancing image description techniques.</p>
      </div>
      <div class="paper">
        <h3>VideoBERT: A Joint Model for Video and Language Representation Learning</h3>
        <p><strong>Analogy:</strong> A film editor aligning visual frames with textual context.</p>
        <p><strong>Novelty & Interest:</strong> Adapts BERT to video, training with masked prediction tasks.</p>
        <p><strong>Conclusion:</strong> Enhances video representation learning, supporting video captioning and forecasting.</p>
      </div>
      <div class="paper">
        <h3>LLaMA: Open and Efficient Foundation Language Models</h3>
        <p><strong>Analogy:</strong> Building a library with publicly available books instead of rare manuscripts.</p>
        <p><strong>Novelty & Interest:</strong> Uses massive public datasets, challenging reliance on proprietary data.</p>
        <p><strong>Conclusion:</strong> Democratizes language modeling while achieving state-of-the-art results.</p>
      </div>
      <div class="paper">
        <h3>Conformer: Convolution-augmented Transformer for Speech Recognition</h3>
        <p><strong>Analogy:</strong> A musician reading both the overall score and the fine details of each note.</p>
        <p><strong>Novelty & Interest:</strong> Interleaves self-attention with convolutional layers for speech processing.</p>
        <p><strong>Conclusion:</strong> Achieves state-of-the-art accuracy on LibriSpeech.</p>
      </div>
    </section>
    
    <footer>
      <p>Back to <a href="index.html">Home</a></p>
    </footer>
  </div>
</body>
</html>
